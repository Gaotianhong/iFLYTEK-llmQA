{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip;\n",
      "python -u cn_clip/eval/extract_features.py     --extract-image-feats     --extract-text-feats     --image-data=\"../datapath/datasets/VQA/lmdb/test/imgs\"     --text-data=\"../datapath/datasets/VQA/test_texts.jsonl\"     --img-batch-size=32     --text-batch-size=32     --context-length=52     --resume=../datapath/experiments/vqa_finetune_vit-h-14_roberta-large_bs48_1gpu/checkpoints/epoch_latest.pt     --vision-model=ViT-H-14     --text-model=RoBERTa-wwm-ext-large-chinese\n",
      "\n",
      "Params:\n",
      "  context_length: 52\n",
      "  debug: False\n",
      "  extract_image_feats: True\n",
      "  extract_text_feats: True\n",
      "  image_data: ../datapath/datasets/VQA/lmdb/test/imgs\n",
      "  image_feat_output_path: None\n",
      "  img_batch_size: 32\n",
      "  precision: amp\n",
      "  resume: ../datapath/experiments/vqa_finetune_vit-h-14_roberta-large_bs48_1gpu/checkpoints/epoch_latest.pt\n",
      "  text_batch_size: 32\n",
      "  text_data: ../datapath/datasets/VQA/test_texts.jsonl\n",
      "  text_feat_output_path: None\n",
      "  text_model: RoBERTa-wwm-ext-large-chinese\n",
      "  vision_model: ViT-H-14\n",
      "Loading vision model config from cn_clip/clip/model_configs/ViT-H-14.json\n",
      "Loading text model config from cn_clip/clip/model_configs/RoBERTa-wwm-ext-large-chinese.json\n",
      "Preparing image inference dataset.\n",
      "Preparing text inference dataset.\n",
      "Begin to load model checkpoint from ../datapath/experiments/vqa_finetune_vit-h-14_roberta-large_bs48_1gpu/checkpoints/epoch_latest.pt.\n",
      "=> loaded checkpoint '../datapath/experiments/vqa_finetune_vit-h-14_roberta-large_bs48_1gpu/checkpoints/epoch_latest.pt' (epoch 5 @ 535 steps)\n",
      "Make inference for texts...\n",
      "100%|███████████████████████████████████████████| 13/13 [00:04<00:00,  2.91it/s]\n",
      "392 text features are stored in ../datapath/datasets/VQA/test_texts.txt_feat.jsonl\n",
      "Make inference for images...\n",
      "100%|███████████████████████████████████████████| 59/59 [01:55<00:00,  1.95s/it]\n",
      "1884 image features are stored in ../datapath/datasets/VQA/test_imgs.img_feat.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "DATAPATH=\"../datapath\"\n",
    "# 为测试集图片池和query文本计算特征\n",
    "dataset_name=\"VQA\"\n",
    "split=\"test\" # 指定计算valid或test集特征\n",
    "resume=f\"{DATAPATH}/experiments/vqa_finetune_vit-h-14_roberta-large_bs48_1gpu/checkpoints/epoch_latest.pt\"\n",
    "\n",
    "run_command = \"export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip;\" + \\\n",
    "f\"\"\"\n",
    "python -u cn_clip/eval/extract_features.py \\\n",
    "    --extract-image-feats \\\n",
    "    --extract-text-feats \\\n",
    "    --image-data=\"{DATAPATH}/datasets/{dataset_name}/lmdb/{split}/imgs\" \\\n",
    "    --text-data=\"{DATAPATH}/datasets/{dataset_name}/{split}_texts.jsonl\" \\\n",
    "    --img-batch-size=32 \\\n",
    "    --text-batch-size=32 \\\n",
    "    --context-length=52 \\\n",
    "    --resume={resume} \\\n",
    "    --vision-model=ViT-H-14 \\\n",
    "    --text-model=RoBERTa-wwm-ext-large-chinese\n",
    "\"\"\"\n",
    "print(run_command.lstrip())\n",
    "!{run_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip;\n",
      "python -u cn_clip/eval/make_topk_predictions.py     --image-feats=\"../datapath/datasets/VQA/test_imgs.img_feat.jsonl\"     --text-feats=\"../datapath/datasets/VQA/test_texts.txt_feat.jsonl\"     --top-k=1     --eval-batch-size=32768     --output=\"../datapath/datasets/VQA/test_predictions.jsonl\"\n",
      "\n",
      "Params:\n",
      "  eval_batch_size: 32768\n",
      "  image_feats: ../datapath/datasets/VQA/test_imgs.img_feat.jsonl\n",
      "  output: ../datapath/datasets/VQA/test_predictions.jsonl\n",
      "  text_feats: ../datapath/datasets/VQA/test_texts.txt_feat.jsonl\n",
      "  top_k: 1\n",
      "Begin to load image features...\n",
      "1884it [00:00, 3138.44it/s]\n",
      "Finished loading image features.\n",
      "Begin to compute top-1 predictions for texts...\n",
      "392it [00:04, 82.94it/s] \n",
      "Top-1 predictions are saved in ../datapath/datasets/VQA/test_predictions.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 进行KNN检索，为测试集每个query，匹配特征余弦相似度最高的top-10商品图片\n",
    "run_command = \"export PYTHONPATH=${PYTHONPATH}:`pwd`/cn_clip;\" + \\\n",
    "f\"\"\"\n",
    "python -u cn_clip/eval/make_topk_predictions.py \\\n",
    "    --image-feats=\"{DATAPATH}/datasets/{dataset_name}/{split}_imgs.img_feat.jsonl\" \\\n",
    "    --text-feats=\"{DATAPATH}/datasets/{dataset_name}/{split}_texts.txt_feat.jsonl\" \\\n",
    "    --top-k=1 \\\n",
    "    --eval-batch-size=32768 \\\n",
    "    --output=\"{DATAPATH}/datasets/{dataset_name}/{split}_predictions.jsonl\"\n",
    "\"\"\"\n",
    "print(run_command.lstrip())\n",
    "!{run_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched_queries=392\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def match_first_text(query_text, test_texts):\n",
    "    for test in test_texts:\n",
    "        if query_text == test['text']:\n",
    "            return test['text_id']\n",
    "    return None\n",
    "\n",
    "\n",
    "DATAPATH = \"../datapath\"\n",
    "# 为测试集图片池和query文本计算特征\n",
    "split = \"test\"  # 指定计算valid或test集特征\n",
    "\n",
    "# query_path = f\"{DATAPATH}/datasets/{dataset_name}/query.json\"\n",
    "query_path = f\"{DATAPATH}/result/result_final1.json\"\n",
    "query_filter_path = f\"{DATAPATH}/datasets/{dataset_name}/query_filter.json\"\n",
    "test_texts_path = f\"{DATAPATH}/datasets/{dataset_name}/test_texts.jsonl\"\n",
    "output_path = f\"{DATAPATH}/datasets/{dataset_name}/{split}_predictions.jsonl\"\n",
    "os.makedirs(f\"{DATAPATH}/result\", exist_ok=True)\n",
    "\n",
    "\n",
    "test_texts = []\n",
    "with open(test_texts_path, 'r') as f:\n",
    "    for line in f:\n",
    "        test_texts.append(json.loads(line))\n",
    "\n",
    "# Read query.json\n",
    "with open(query_path, 'r') as f:\n",
    "    query_data = json.load(f)\n",
    "\n",
    "# Extract relevant queries with the pattern \"请匹配到与(.+?)最相关的图片\"\n",
    "query_filter = []\n",
    "match_number = 0\n",
    "for query in query_data:\n",
    "    match = re.search(r'请匹配到与(.+?)最相关的图片', query[\"question\"])\n",
    "    if match:\n",
    "        query_text = match.group(1).strip()\n",
    "        text_id = match_first_text(query_text, test_texts)  # Find the first matching text_id\n",
    "        match_number += 1\n",
    "        # print(text_id)\n",
    "        with open(output_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                record = json.loads(line)\n",
    "                if record[\"text_id\"] == text_id:\n",
    "                    target_image_id = record[\"image_ids\"][0]\n",
    "                    query[\"answer\"] = target_image_id\n",
    "                    break\n",
    "    else:\n",
    "        query_filter.append(query)\n",
    "\n",
    "print(f'matched_queries={match_number}')\n",
    "\n",
    "result_path = f\"{DATAPATH}/result/result_update.json\"\n",
    "with open(result_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(query_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(query_filter_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(query_filter, json_file, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# 查看image\n",
    "image_ids = ['gpgpovggzaeffilclosz.jpg']\n",
    "\n",
    "lmdb_imgs = \"../datapath/datasets/VQA/lmdb/train/imgs\"\n",
    "env_imgs = lmdb.open(lmdb_imgs, readonly=True, create=False, lock=False, readahead=False, meminit=False)\n",
    "txn_imgs = env_imgs.begin(buffers=True)\n",
    "for image_id in image_ids:\n",
    "    image_b64 = txn_imgs.get(\"{}\".format(image_id).encode('utf-8')).tobytes()\n",
    "    img = Image.open(BytesIO(base64.urlsafe_b64decode(image_b64)))\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
